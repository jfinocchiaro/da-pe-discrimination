\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

%     to avoid loading the natbib package, add option nonatbib:
\PassOptionsToPackage{numbers, sort, compress}{natbib}
\usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{draft} % weird pdfendlink error
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{0}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\alex}[1]{\mynote{green!80!blue}{[RF: #1]}}
\newcommand{\alext}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{cyan}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{cyan!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{\textcolor{gray}{BTW: #1}}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\relint}[1]{\mathrm{relint}(#1)}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{cc\,dim}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\propdis}{\mu}
\newcommand{\affhull}{\mathrm{affhull}}
\newcommand{\epi}{\mathrm{epi}}


\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Sc}{\mathcal{S}}  % jessie, feel free to redef, just not \S :-)
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\ellbar}{\underline{\ell}}
\newcommand{\lbar}{\underline{L}} % couldn't do L* while proofreading...
\newcommand{\iden}{\mathrm{iden}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\exploss}[3]{\E_{#3} #1(#2,Y)}
\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\ones}{\mathbbm{1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\usepackage{thmtools, thm-restate}
%\declaretheorem{corollary}


\title{Discrimination from domain adaptation and property elicitation perspectives}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Alexander Ritchie\\
%   Department of Computer Science\\
   University of Michigan\\
  % Address \\
   \texttt{aritch@umich.edu} 
%   \And
%   Bo Waggoner\\
%%   Department of Computer Science\\
%   CU Boulder \\
%  % Address \\
%   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Setting}
Consider predictions and labels that are either discrete or continuous.
I'm going to use my least favorite example of criminal justice because it's one of the most prominent areas assigning risk scores to human beings, but if a paper comes out of this, I'd prefer a different example, since I largely think this practice shouldn't be glorified slash given any spotlight.

Losses for discrete predictions and labels might correspond to predicting whether or not someone will commit a crime compared to whether they actually do or not.
These are typically hard to optimize, so we aim to optimize a continuous, consistent surrogate instead.

Losses with continuous predictions and discrete labels might take a risk score as prediction and assign loss comparing this risk score to whether or not the person actually committed the crime.

The continuous labels cases are similar, expect predictions are compared to true risk, which often cannot actually be determined.
In that sense, we want to live in a discrete outcome setting, but there are times we might want to generalize this.

Consider the \emph{feature space} $\X$ and \emph{label space} $\Y$.
Suppose data is drawn over some distributions $D^a$ and $D^d$ over $\X \times\ Y$.

\section{Domain Adaptation}

\section{Property elicitation}

A property naturally corresponds to the ``suggested'' report for a given task.
\begin{definition}[Property, elicits, level set]
	A \emph{property} $\Gamma: \simplex \to 2^{\R} \setminus \{ \emptyset\}$ is a (possibly set-valued) function mapping probability distributions to reports.
	We say a loss $L : \R \times\Y \to \reals$ \emph{elicits} a property $\Gamma$ if,
	\begin{equation}
	\forall p \in \simplex, \;\; \Gamma(p) = \argmin_{r \in \R} \exploss{L}{r}{p}~.~
	\end{equation}
	Moreover, the set-valued inverse of a property, its \emph{level set} is given by $\Gamma_r = \{p \in \simplex : r \in \Gamma(p)\}$.
\end{definition}
We say a property if \emph{finite} if $|\R| < \infty$.

\begin{theorem}[\citep{lambert2009eliciting}]
	The level sets of a finite elicitable property a convex (and closed.)
\end{theorem}

\subsection{Thoughts on fairness here}
Individual fairness gives a function $f : \X \times G \to \simplex$\jessiet{Unclear what assumptions are needed on $f$} mapping 

\paragraph{Individual fairness}
An algorithm would plausibly be individually fair if $f(x, a) = f(x, d)$ for all $x \in \X$.\jessie{I don't like this definition; if $x$ captures group relevant features, then two equally qualified people might not actually have $x_1 = x_2$ like it should be... should multiple properties be used here?}
We can lighten this restriction to say an algorithm is classification individually fair (CIF) if $\Gamma(f(x, a)) = \Gamma(f(x,d))$ for all $x \in \X$.

\paragraph{Group fairness}
We use the notion of different properties for each group.
Consider $p^a$ and $p^d$ to be $\E_X D^a$ and $D^d$ respectively.
Two properties $\Gamma^a$ and $\Gamma^d$ are \emph{classification group fair} if $\E_X \Gamma^a(p^a) = \E_X \Gamma^d(p^d)$.

\paragraph{Simultaneous fairness}
Two properties $\Gamma^a$ and $\Gamma^d$ are \emph{classification fair} if $\E_X \Gamma^a(f(X, a)) = \E_X \Gamma^d(f(X, d))$.

We can also generalize these notions to continuous estimation settings.

\section{Fairness}
Interested in understanding the probability of disparate treatment.
Question: is comparative fairness a correct notion?  
Should we focus on discrimination rather than fairness?


\subsection{Individual Fairness}
\subsection{Group Fairness}

\begin{ack}
whoever
\end{ack}

\bibliographystyle{plainnat}
\bibliography{diss,extra}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
